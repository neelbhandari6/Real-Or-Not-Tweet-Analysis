{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Using CNNs to classify disaster tweets\n",
    "Let's look at the training data first. We can use the pandas module to read our training data and store it in a dataframe. When we view the first few rows of the dataframe, we can see that there are 5 columns, and 2 columns have missing values. We then drop these columns as they aren't really useful for classification. \n",
    "Before we send data into the model, we need to convert it into a ....\n",
    "## Text Preprocessing\n",
    "So we begin text preprocessing by converting the 'text' column of the dataframe into a list. Each tweet is an element of this list. Now, from each tweet we remove numbers using regex pattern matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "np.random.seed(500) \n",
    "train=pd.DataFrame(pd.read_csv('../input/nlp-getting-started/train.csv'))\n",
    "train.head()\n",
    "train=train.dropna(axis=1)\n",
    "tweets=train['text'].to_list()\n",
    "# print(train)\n",
    "# print(tweets)\n",
    "nonums=[]\n",
    "for tweet in tweets:\n",
    "    nonums.append(re.sub(r'\\d+', '', tweet))\n",
    "# print(nonums)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "When you print the output, you can see that there are a bunch of URLs in the tweets as well. We remove these by matching any text that begins with 'http\\'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\r\n",
      "  Downloading contractions-0.0.24-py2.py3-none-any.whl (3.2 kB)\r\n",
      "Collecting textsearch\r\n",
      "  Downloading textsearch-0.0.17-py2.py3-none-any.whl (7.5 kB)\r\n",
      "Requirement already satisfied: pyahocorasick in /opt/conda/lib/python3.7/site-packages (from textsearch->contractions) (1.4.0)\r\n",
      "Requirement already satisfied: Unidecode in /opt/conda/lib/python3.7/site-packages (from textsearch->contractions) (1.1.1)\r\n",
      "Installing collected packages: textsearch, contractions\r\n",
      "Successfully installed contractions-0.0.24 textsearch-0.0.17\r\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "p=re.compile(r'\\<http.+?\\>', re.DOTALL)\n",
    "\n",
    "tweetswithouturls=[]\n",
    "for tweet in nonums:\n",
    "    tweetswithouturls.append(re.sub(r\"http\\S+\", \"\", tweet))\n",
    "# print(tweetswithouturls)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Next, we replace contraced forms of words like 'don't' and 'can't' with their expanded forms 'do not' and 'cannot'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import contractions\n",
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "nocontractions=[]\n",
    "for tweet in tweetswithouturls:\n",
    "    \n",
    "\n",
    "    nocontractions.append(replace_contractions(tweet))\n",
    "# print(nocontractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "We then tokenise each tweet. With tokenising, we transform each tweet into a list, with each element of this tweet list being each word in the tweet. These words are called tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "tokens = [word_tokenize(sen) for sen in nocontractions]\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "We see that some of the tokens are punctiuation indicators like  .  ,  ?  , ! . These aren't necessary as well, as we need to understand the text in terms of the words and their context only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "nopunct=[]\n",
    "for listt in tokens:\n",
    "    nopunct.append(remove_punctuation(listt))\n",
    "# print(nopunct)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "We then remove any characters which are not ascii characters. So we retain only A-Z as we removed numbers already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, unicodedata\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "onlyascii=[]\n",
    "for listt in nopunct:\n",
    "    onlyascii.append(remove_non_ascii(listt))\n",
    "# print(onlyascii)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "To maintain uniformity we convert all letters to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "lower=[]\n",
    "for listt in onlyascii:\n",
    "    lower.append(to_lowercase(listt))\n",
    "# print(lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "We then remove stopwords, which don't necessarily add to convey the main idea of the tweet. These include words like i, our, of, for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "    \n",
    "nostopwords=[]\n",
    "for listt in lower:\n",
    "    nostopwords.append(remove_stopwords(listt))\n",
    "# print(nostopwords)\n",
    "# print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Here, we lemmatize the words to use only the root forms of the words, unless they're nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "import collections\n",
    "tag_map = collections.defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "Final_words = []\n",
    "word_Lemmatized = WordNetLemmatizer()\n",
    "for entry in nostopwords:\n",
    "#     print(entry)\n",
    "    words=[]\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    \n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        \n",
    "        word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "#         print(word_Final)\n",
    "        words.append(word_Final)\n",
    "    Final_words.append(words)\n",
    "# print(Final_words)\n",
    "# for entry in nostopwords:\n",
    "#     print(pos_tag(entry))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Let's now add these preprocessed tokens next to the tweets in the dataframe. We add them as sentences or a string of tokens in one column called \"Text_Final\". Then in another column called \"tokens\", we add the them as a list of tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_Final</th>\n",
       "      <th>tokens</th>\n",
       "      <th>target</th>\n",
       "      <th>Disaster</th>\n",
       "      <th>Not a Disaster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "      <td>[deed, reason, earthquake, may, allah, forgive...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>[resident, ask, shelter, place, notify, office...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "      <td>[people, receive, wildfire, evacuation, order,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>get sent photo ruby alaska smoke wildfires pou...</td>\n",
       "      <td>[get, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Text_Final  \\\n",
       "0         deed reason earthquake may allah forgive u   \n",
       "1              forest fire near la ronge sask canada   \n",
       "2  resident ask shelter place notify officer evac...   \n",
       "3  people receive wildfire evacuation order calif...   \n",
       "4  get sent photo ruby alaska smoke wildfires pou...   \n",
       "\n",
       "                                              tokens  target  Disaster  \\\n",
       "0  [deed, reason, earthquake, may, allah, forgive...       1         1   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]       1         1   \n",
       "2  [resident, ask, shelter, place, notify, office...       1         1   \n",
       "3  [people, receive, wildfire, evacuation, order,...       1         1   \n",
       "4  [get, sent, photo, ruby, alaska, smoke, wildfi...       1         1   \n",
       "\n",
       "   Not a Disaster  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Text_Final'] = [' '.join(sen) for sen in Final_words]\n",
    "train['tokens'] = Final_words\n",
    "disaster = []\n",
    "notdisaster = []\n",
    "for l in train['target']:\n",
    "    if l == 0:\n",
    "        disaster.append(0)\n",
    "        notdisaster.append(1)\n",
    "    elif l == 1:\n",
    "        disaster.append(1)\n",
    "        notdisaster.append(0)\n",
    "train['Disaster']= disaster\n",
    "train['Not a Disaster']= notdisaster\n",
    "\n",
    "train = train[['Text_Final', 'tokens', 'target', 'Disaster', 'Not a Disaster']]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "We then repeat all the processing steps for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeating for test\n",
    "test=pd.DataFrame(pd.read_csv('../input/nlp-getting-started/test.csv'))\n",
    "test.head()\n",
    "test=test.dropna(axis=1)\n",
    "tweetstest=test['text'].to_list()\n",
    "# print(train)\n",
    "# print(tweets)\n",
    "nonumstest=[]\n",
    "for tweet in tweetstest:\n",
    "    nonumstest.append(re.sub(r'\\d+', '', tweet))\n",
    "# print(nonums)\n",
    "\n",
    "\n",
    "tweetswithouturlstest=[]\n",
    "for tweet in nonumstest:\n",
    "    tweetswithouturlstest.append(re.sub(r\"http\\S+\", \"\", tweet))\n",
    "# print(tweetswithouturlstest)\n",
    "\n",
    "nocontractionstest=[]\n",
    "for tweet in tweetswithouturlstest:\n",
    "    \n",
    "    nocontractionstest.append(replace_contractions(tweet))\n",
    "# print(nocontractionstest)\n",
    "tokenstest = [word_tokenize(sen) for sen in nocontractionstest]\n",
    "# print(tokenstest)\n",
    "\n",
    "nopuncttest=[]\n",
    "for listt in tokenstest:\n",
    "    nopuncttest.append(remove_punctuation(listt))\n",
    "# print(nopuncttest)\n",
    "\n",
    "onlyasciitest=[]\n",
    "for listt in nopuncttest:\n",
    "    onlyasciitest.append(remove_non_ascii(listt))\n",
    "# print(onlyasciitest)\n",
    "\n",
    "lowertest=[]\n",
    "for listt in onlyasciitest:\n",
    "    lowertest.append(to_lowercase(listt))\n",
    "# print(lowertest)\n",
    "\n",
    "nostopwordstest=[]\n",
    "for listt in lowertest:\n",
    "    nostopwordstest.append(remove_stopwords(listt))\n",
    "# print(nostopwordstest)\n",
    "Finalwordstest=[]\n",
    "for entry in nostopwordstest:\n",
    "#     print(entry)\n",
    "    words=[]\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    \n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        \n",
    "        word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "#         print(word_Final)\n",
    "        words.append(word_Final)\n",
    "    Finalwordstest.append(words)\n",
    "# print(Finalwordstest)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Next we add the tokenised test tweets as sentences in one column called \"Text_Final\" and as tokens in another column called \"tokens\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>Text_Final</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "      <td>[happen, terrible, car, crash]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>heard earthquake different city stay safe ever...</td>\n",
       "      <td>[heard, earthquake, different, city, stay, saf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>forest fire spot pond geese flee across street...</td>\n",
       "      <td>[forest, fire, spot, pond, geese, flee, across...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse light spokane wildfire</td>\n",
       "      <td>[apocalypse, light, spokane, wildfire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "      <td>[typhoon, soudelor, kill, china, taiwan]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  \\\n",
       "0   0                 Just happened a terrible car crash   \n",
       "1   2  Heard about #earthquake is different cities, s...   \n",
       "2   3  there is a forest fire at spot pond, geese are...   \n",
       "3   9           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                          Text_Final  \\\n",
       "0                          happen terrible car crash   \n",
       "1  heard earthquake different city stay safe ever...   \n",
       "2  forest fire spot pond geese flee across street...   \n",
       "3                  apocalypse light spokane wildfire   \n",
       "4                 typhoon soudelor kill china taiwan   \n",
       "\n",
       "                                              tokens  \n",
       "0                     [happen, terrible, car, crash]  \n",
       "1  [heard, earthquake, different, city, stay, saf...  \n",
       "2  [forest, fire, spot, pond, geese, flee, across...  \n",
       "3             [apocalypse, light, spokane, wildfire]  \n",
       "4           [typhoon, soudelor, kill, china, taiwan]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Text_Final'] = [' '.join(sen) for sen in Finalwordstest]\n",
    "test['tokens'] = Finalwordstest\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "We then create a bag of training words as a list called \"all_training_words\". This contains all the words in all of the training tweets as one list. We create another array called \"training_sentences_length\" which stores a list of lengths of each tweet.\n",
    "Then a list called \"TRAINING_VOCAB\" is created to store the set of unique words in all of the tweets in the training set sorted alphabetically. So this gives us the vocabulary we're dealing with to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "An alternate form of counting and vectorising is using tf-idf, where the frequency of a word is compared with frequency across all documents, so its frequency can be attributed to being because of its existence in a class. This vectorisation has been used for SVM which i've used at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68655 words total, with a vocabulary size of 14824\n",
      "Max sentence length is 23\n",
      "(7613, 14800)\n",
      "(7613, 14800)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "all_training_words = [word for tokens in train[\"tokens\"] for word in tokens]\n",
    "\n",
    "# count_vect = CountVectorizer()\n",
    "# X_train_counts = count_vect.fit_transform(all_training_words)\n",
    "# print(X_train_counts)\n",
    "training_sentence_lengths = [len(tokens) for tokens in train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))\n",
    "### for svm split data\n",
    "\n",
    "##tfidf\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train[\"Text_Final\"])\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "print(X_train_tf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "We repeat the above process for test data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29793 words total, with a vocabulary size of 8795\n",
      "Max sentence length is 22\n"
     ]
    }
   ],
   "source": [
    "all_test_words = [word for tokens in test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in test[\"tokens\"]]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "We then use the Keras Tokeniser to create a set of indices for each word. So since TRAINING_VOCAB represents the set of unique words, the length of this list gives the max number of indices required. So each tweet is now represented as a set of numbers with each word replaced by an index. So what the fit_on_texts method does is, it looks at the frequencies of words appearing in all the tweets and gives a lower index if its frequency is higher. So a word like 'injured' might occur most frequently in this data set and might be given the index 1. Now the fit_to_sentence method replaces each word in every tweet with the index it was fit with. Train_word_index is a dictionary containing the index mapped to the corresponding word. So its length gives set of unique words. So now we have lists of indices as tweets. But each tweet is of different length, so we pad with zeroes. Now each tweet is a sequence of indices and its length is 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14831 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D,MaxPooling1D, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "\n",
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(train[\"Text_Final\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(train[\"Text_Final\"].tolist())\n",
    "train_word_index = tokenizer.word_index\n",
    "print(\"Found %s unique tokens.\" % len(train_word_index))\n",
    "train_cnn_data = pad_sequences(training_sequences, \n",
    "                               maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Here, we also tokenise the test data into a sequence of indices as well and also pad the sequences. But the indices used are same as the ones used for the train data. And unseen words are relaced with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3263, 14800)\n",
      "(3263, 14800)\n"
     ]
    }
   ],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test[\"Text_Final\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# print(test_cnn_data.shape)\n",
    "X_test_counts = count_vect.transform(test['Text_Final'])\n",
    "X_test_tf = tf_transformer.transform(X_test_counts)\n",
    "print(X_test_counts.shape)\n",
    "print(X_test_tf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Creating word embeddings\n",
    "Now, we use a trained word2vec model. This model takes in a corpus of text as input, which in this case is the set of words in the tweets in the order in which they appear. This represents a continuous bag of words. It then creates vectors for each word. These vectors now constitute a 300 dimensional vector space with words sharing common context appearing closer together in the vector space. Creating word embeddings this way helps us create a representation for words in their linguistic contexts. So we load the path where the model is stored. We then load the model in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import gensim.downloader as api\n",
    "# # path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
    "path='../input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin'\n",
    "from gensim import models\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(path, binary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "We now construct vectors for each word such that if the word already exists in the word2vec model, the vector for that word is used, and if it is not, a random vector is used. So each vector length is 300. So train_embedding_weights contains the vectors for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14832, 300)\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "We now define our CNN with an embedding layer. So any data that goes into this model is embedded through word2vec like we just did. So for test data this happens here, based on the train_embedding_matrix as weights, an embedding layer is created for the test input. This embedding layer is a layer of word2vec vectors of the test data. This layer acts as input to the convolutional layer. So for the convolutional layer is of depth 5. The 5 filters are of sizes 2,3,4,5,6. So the embedded input passes through each filter and a global max pooling layer that makes the number of parameters that pass through the subsequent layers smaller, by taking max parameter in a region. A relu activation function is also applied. The resultant is passed through a dropout layer with 10 percent dropout. Then a dense layer that retains 128 parameters and relu activation function is used and then another dropout layer. We end with a final dense layer and a relu activation layer that provides probabilities of 'disaster' and 'not disaster'. We then define loss function and the optimiser for backtracking and updating the weights. We return this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    \n",
    "    \n",
    "    preds = Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "So here we assign the labels to y_train for training. We then also assign the padded tokenised tweets of training data to x_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['Disaster', 'Not a Disaster']\n",
    "y_train = train[label_names].values\n",
    "\n",
    "\n",
    "# print(y_train)\n",
    "x_train = train_cnn_data"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "We define number of epochs and batch size for training.\n",
    "We also define the early stopping criteria so training stops when validation loss reaches a minimum. This prevents overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "num_epochs = 4 #3 is enough but just testing\n",
    "batch_size = 24\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "We create an instance of our model and pass the training weights, max sequence length, no. of unique words, embedding dimension, and the no. of output labels to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6851 samples, validate on 762 samples\n",
      "Epoch 1/4\n",
      "6851/6851 [==============================] - 7s 1ms/step - loss: 0.5299 - acc: 0.7418 - val_loss: 0.4609 - val_acc: 0.7999\n",
      "Epoch 2/4\n",
      "6851/6851 [==============================] - 2s 327us/step - loss: 0.4098 - acc: 0.8216 - val_loss: 0.5152 - val_acc: 0.7618\n",
      "Epoch 00002: early stopping\n"
     ]
    }
   ],
   "source": [
    "# for i in range(5):\n",
    "#     print('Trial-',i)\n",
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
    "                len(list(label_names)))\n",
    "\n",
    "hist = model.fit(x_train, y_train, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size,callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "The model's predictions is stored. This is a list of probabilities for both 'disaster' and 'not a disaster'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3263/3263 [==============================] - 0s 144us/step\n",
      "[[0.78391504 0.22957301]\n",
      " [0.9146771  0.07437406]\n",
      " [0.9665908  0.03101815]\n",
      " ...\n",
      " [0.7829081  0.22706304]\n",
      " [0.8481988  0.17335206]\n",
      " [0.820558   0.15755576]]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Next from the probability scores, we assign 1 if 'disaster' class is a higher probability and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1, 0]\n",
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "# print(prediction_labels)\n",
    "i=1\n",
    "# for p in prediction_labels:\n",
    "#     print(i,'-',p)\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "We now add the predictions in the dataframe. We also write the predictions into the submissions file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['target']=prediction_labels\n",
    "# print(test[['tokens','target']])\n",
    "submissions=pd.DataFrame(pd.read_csv('../input/nlp-getting-started/sample_submission.csv'))\n",
    "# submissions['target']=prediction_labels\n",
    "# print(submissions)\n",
    "# submissions.to_csv('/kaggle/working/submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Some random code below pls ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submissions=pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\n",
    "# comparewithnb=pd.DataFrame(pd.read_csv('../input/comparewithnb/filename11.csv'))\n",
    "# cwnb=comparewithnb['0'].to_list()\n",
    "# print(len(cwnb))\n",
    "# count=0\n",
    "# mismatch=[]\n",
    "# for i in range(3263):\n",
    "#     if(cwnb[i]==prediction_labels[i]):\n",
    "#         count+=1\n",
    "#     else:\n",
    "#         mismatch.append(i)\n",
    "# print(count)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2610\n"
     ]
    }
   ],
   "source": [
    "testlabels=pd.DataFrame(pd.read_csv('../input/testlabels2/submission.csv'))\n",
    "labels=testlabels['target'].to_list()\n",
    "count=0\n",
    "mismatch=[]\n",
    "for i in range(3263):\n",
    "    if(labels[i]==prediction_labels[i]):\n",
    "        count+=1\n",
    "    else:\n",
    "        mismatch.append(i)\n",
    "print(count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Here, I've implemented the SVM, which tries to divide datapoints into classes by using a hyperplane. This hyperplane must be as distant from the two classes as possible. The support vectors are the points closest to the hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm,metrics\n",
    "\n",
    "train_model=svm.SVC().fit(X_train_tf, train[\"target\"].values)\n",
    "predictions=train_model.predict(X_test_tf)\n",
    "\n",
    "# SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "# hist2=SVM.fit(X_train_tf,train_y)\n",
    "# predictions_SVM = SVM.predict(X_test_tf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "SVM does seem to do better than CNNs, but I will look into this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2616\n"
     ]
    }
   ],
   "source": [
    "##SVM\n",
    "count=0\n",
    "mismatch=[]\n",
    "for i in range(3263):\n",
    "    if(labels[i]==predictions[i]):\n",
    "        count+=1\n",
    "    else:\n",
    "        mismatch.append(i)\n",
    "print(count)\n",
    "submissions['target']=predictions\n",
    "\n",
    "submissions.to_csv('/kaggle/working/submission.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
